# Chat And Text Completion Model:

## 1. ChatOllama or ChatModel:

- It internally calls `/api/chat`.

- Works with structured chat interactions using mesages. `langchain_core.messages`

- Suitable for multi-turn converstation, where context is maintained accross messages.


```python
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, AIMessage, HumanMessage


chat_llm = ChatOllama(model='llama3:latest')

if __name__ == "__main__":
    ip = input("Enter your prompt: ")

    # chat model
    # --------------------------------------------------------------------------------------------------------
    res = chat_llm.invoke((
        SystemMessage(content="Your name is a ollama translator, your job is to translate to english"),
        HumanMessage("Bom dia?"),
        AIMessage("Bom dia translated to english is: heyy how are you doing?"),
        HumanMessage(ip),
    ))
    print(res)
    print(type(res)) # <class 'langchain_core.messages.ai.AIMessage'>
    # --------------------------------------------------------------------------------------------------------

```

Here, ``SystemMessage, HumanMessage, AIMessage`` are the message for ChatModel for different roles.

## 2. OllamaLLM or Text Completion Model:

- Calls the Completion API i.e. ``api/generate``

- Works with single-turn generation using a plain prompt.

- Suitable for tasks like summarization, text completion, and Q&A.

```python
from langchain_community.llms import OllamaLLM

llm_model = OllamaLLM(model="mistral")
response = llm_model.invoke("Explain Ollama in simple terms.")
print(response)
print(type(response)) # str
```
